# Toward World's Most Comprehensive Curated List of LLM Related Papers & Repositories[[Project Page](https://shure-dev.github.io/)][[Notion Table](https://potent-twister-29f.notion.site/b0fc32542854456cbde923e0adb48845?v=e2d14d2ef0c848f5a1d5b71f9977d7c5&pvs=4)]

## Target fields
CoT / VLM / Quantization / Grounding / Text2IMG&VID / Prompt / Reasoning / Robot / Agent / Planning / RL / Feedback / InContextLearning / InstructionTuning / PEFT / RLHF / RAG / Embodied / VQA / Hallucination / Diffusion / Scaling / ContextWindow / WorldModel / Memory / ZeroShot / RoPE / Speech

## Mission
Our mission is to provide a wide selection of the latest, high-quality papers in the rapidly evolving field of LLM related that are organized and essential for researchers striving to stay abreast of developments in the field.

## Notion Table 
https://potent-twister-29f.notion.site/b0fc32542854456cbde923e0adb48845?v=e2d14d2ef0c848f5a1d5b71f9977d7c5&pvs=74
[<img width="1357" alt="image" src="https://github.com/shure-dev/Awesome-LLM-Papers-Toward-AGI/assets/61527175/84447574-1134-4b0d-8442-f509162da10e">](https://potent-twister-29f.notion.site/b0fc32542854456cbde923e0adb48845?v=e2d14d2ef0c848f5a1d5b71f9977d7c5&pvs=4)

## ðŸ”¥We are working on only notion now
This repo is not updated anymore

## Star History

[![Star History Chart](https://api.star-history.com/svg?repos=shure-dev/Awesome-LLM-Papers-Toward-AGI&type=Date)](https://star-history.com/#shure-dev/Awesome-LLM-Papers-Toward-AGI&Date)



## Awesome repos
[Awesome-Multimodal-Large-Language-Models](https://github.com/BradyFU/Awesome-Multimodal-Large-Language-Models)<br>
[Awesome-LLM-Robotics](https://github.com/GT-RIPL/Awesome-LLM-Robotics)<br>
[Awesome-Multimodal-Reasoning](https://github.com/atfortes/Awesome-Multimodal-Reasoning)<br>
[LLM-Reasoning-Papers](https://github.com/atfortes/LLM-Reasoning-Papers)<br>
[LLMSurvey](https://github.com/RUCAIBox/LLMSurvey)<br>
[Awesome_Multimodel_LLM](https://github.com/Atomic-man007/Awesome_Multimodel_LLM)<br>
## Awesome surveys
[A Survey of Large Language Models](https://arxiv.org/abs/2303.18223)<br>
[A Survey of Chain of Thought Reasoning: Advances, Frontiers and Future](https://arxiv.org/abs/2309.15402)<br>
[Towards Reasoning in Large Language Models: A Survey](https://arxiv.org/abs/2212.10403)<br>
[A Survey on Large Language Model based Autonomous Agents](https://arxiv.org/abs/2308.11432)<br>
[The Rise and Potential of Large Language Model Based Agents: A Survey](https://arxiv.org/abs/2309.07864)<br>
[Toward General-Purpose Robots via Foundation Models: A Survey and Meta-Analysis](https://arxiv.org/abs/2312.08782)<br>
[Language-conditioned Learning for Robotic Manipulation: A Survey](https://arxiv.org/abs/2312.10807)
[Foundation Models in Robotics: Applications, Challenges, and the Future](https://arxiv.org/abs/2312.07843)<br>
[The Development of LLMs for Embodied Navigation](https://arxiv.org/abs/2311.00530)<br>
[LLM Powered Autonomous Agents](https://lilianweng.github.io/posts/2023-06-23-agent/?ref=emergentmind)<br>
[Awesome-Embodied-Agent-with-LLMs](https://github.com/zchoi/Awesome-Embodied-Agent-with-LLMs)
## LLM
| Subcategory | Models | Linked Title | Publication Date |
|-------------|--------|--------------|-----------------|
| Open sourced LLM | BERT | [BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/abs/1810.04805) | 11 Oct 2018 |
|  | T5 | [Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer](https://arxiv.org/abs/1910.10683) | 23 Oct 2019 |
|  | LLaMA | [LLaMA: Open and Efficient Foundation Language Models](https://arxiv.org/abs/2302.13971) | 27 Feb 2023 |
|  | OpenFlamingo | [OpenFlamingo: An Open-Source Framework for Training Large Autoregressive Vision-Language Models](https://arxiv.org/abs/2308.01390) | 2 Aug 2023 |
|  | InstructBLIP | [InstructBLIP: Towards General-purpose Vision-Language Models with Instruction Tuning](https://arxiv.org/abs/2305.06500) | 11 May 2023 |
|  | ChatBridge | [ChatBridge: Bridging Modalities with Large Language Model as a Language Catalyst](https://arxiv.org/abs/2305.16103) | 25 May 2023 |
| Closed sourced LLM | GPT3 | [Language Models are Few-Shot Learners](https://arxiv.org/abs/2005.14165) | 28 May 2020 |
|  | GPT4 | [GPT-4 Technical Report](https://arxiv.org/abs/2303.08774) | 15 Mar 2023 |
| Instruction Turning | InstructGPT | [Training language models to follow instructions with human feedback](https://arxiv.org/abs/2203.02155) | 4 Mar 2022 |
|  | LLaVA | [Visual Instruction Tuning](https://arxiv.org/abs/2304.08485) | 17 Apr 2023 |
|  | LLaVA | [Visual Instruction Tuning](https://arxiv.org/abs/2304.08485) | 17 Apr 2023 |
|  | MiniGPT-4 | [MiniGPT-4: Enhancing Vision-Language Understanding with Advanced Large Language Models](https://arxiv.org/abs/2304.10592) | 20 Apr 2023 |
|  | FLAN | [Finetuned Language Models Are Zero-Shot Learners](https://arxiv.org/abs/2109.01652) | 3 Sep 2021 |
|  | LLaMA-adapter | [LLaMA-Adapter: Efficient Fine-tuning of Language Models with Zero-init Attention](https://arxiv.org/abs/2303.16199) | 28 Mar 2023 |
|  | Self-Instruct | [Self-Instruct: Aligning Language Models with Self-Generated Instructions](https://arxiv.org/abs/2212.10560) | 20 Dec 2022 |
|  | RAFT | [RAFT: Adapting Language Model to Domain Specific RAG](https://arxiv.org/abs/2403.10131) | 15 Mar 2024 |
| Vision-LLM | LLaVA | [Visual Instruction Tuning](https://arxiv.org/abs/2304.08485) | 17 Apr 2023 |
|  | GPT4-V OpenFlamingo | [OpenFlamingo: An Open-Source Framework for Training Large Autoregressive Vision-Language Models](https://arxiv.org/abs/2308.01390) | 2 Aug 2023 |
|  | InternGPT | [InternGPT: Solving Vision-Centric Tasks by Interacting with ChatGPT Beyond Language](https://arxiv.org/abs/2305.05662) | 9 May 2023 |
|  | PaLM | [PaLM: Scaling Language Modeling with Pathways](https://arxiv.org/abs/2204.02311) | 5 Apr 2022 |
| Spatial Understanding | Gpt-driver | [GPT-Driver: Learning to Drive with GPT](https://arxiv.org/abs/2310.01415) | 2 Oct 2023 |
|  | Path planners | [Can Large Language Models be Good Path Planners? A Benchmark and Investigation on Spatial-temporal Reasoning](https://arxiv.org/abs/2310.03249) | 5 Oct 2023 |
| Visual Question Answering | CogVLM | [CogVLM: Visual Expert for Pretrained Language Models](https://arxiv.org/abs/2311.03079) | 6 Nov 2023 |
|  | ViperGPT | [ViperGPT: Visual Inference via Python Execution for Reasoning](https://arxiv.org/abs/2303.08128) | 14 Mar 2023 |
|  | VISPROG | [Visual Programming: Compositional visual reasoning without training](https://arxiv.org/abs/2211.11559) | 18 Nov 2022 |
|  | MM-ReAct | [MM-REACT: Prompting ChatGPT for Multimodal Reasoning and Action](https://arxiv.org/abs/2303.11381) | 20 Mar 2023 |
|  | Chameleon | [Chameleon: Plug-and-Play Compositional Reasoning with Large Language Models](https://arxiv.org/abs/2304.09842) | 19 Apr 2023 |
|  | Caption Anything | [Caption Anything: Interactive Image Description with Diverse Multimodal Controls](https://arxiv.org/abs/2305.02677) | 4 May 2023 |
| Temporal Logics | NL2TL | [NL2TL: Transforming Natural Languages to Temporal Logics using Large Language Models](https://arxiv.org/abs/2305.07766) | 12 May 2023 |
| Quantitive Analysis | GPT4Vis | [GPT4Vis: What Can GPT-4 Do for Zero-shot Visual Recognition?](https://arxiv.org/abs/2311.15732) | 27 Nov 2023 |
|  | Gemini vs GPT-4V | [Gemini vs GPT-4V: A Preliminary Comparison and Combination of Vision-Language Models Through Qualitative Cases](https://arxiv.org/abs/2312.15011) | 22 Dec 2023 |
| Survey Papers | A Survey of Large Language Models | [A Survey of Large Language Models](https://arxiv.org/abs/2303.18223) | 31 Mar 2023 |

## In-Context Learning
| Subcategory | Models | Linked Title | Publication Date |
|-------------|--------|--------------|-----------------|
| Chain of Thought | Chain of Thought | [Chain-of-Thought Prompting Elicits Reasoning in Large Language Models](https://arxiv.org/abs/2201.11903) | 28 Jan 2022 |
|  | Tree of Thought | [Tree of Thoughts: Deliberate Problem Solving with Large Language Models](https://arxiv.org/abs/2305.10601) | 17 May 2023 |
|  | Multimodal-CoT | [Multimodal Chain-of-Thought Reasoning in Language Models](https://arxiv.org/abs/2302.00923) | 2 Feb 2023 |
|  | Auto-CoT | [Automatic Chain of Thought Prompting in Large Language Models](https://arxiv.org/abs/2210.03493) | 7 Oct 2022 |
|  | Verify-and-Edit | [Verify-and-Edit: A Knowledge-Enhanced Chain-of-Thought Framework](https://arxiv.org/abs/2305.03268) | 5 May 2023 |
|  | Skeleton-of-Thought | [Skeleton-of-Thought: Large Language Models Can Do Parallel Decoding](https://arxiv.org/abs/2307.15337) | 28 Jul 2023 |
|  | Rethinking with Retrieval | [Rethinking with Retrieval: Faithful Large Language Model Inference](https://arxiv.org/abs/2301.00303) | 31 Dec 2022 |
| Reasoning | Self-Consistency | [Self-Consistency Improves Chain of Thought Reasoning in Language Models](https://arxiv.org/abs/2203.11171) | 21 Mar 2022 |
|  | ReAct | [ReAct: Synergizing Reasoning and Acting in Language Models](https://arxiv.org/abs/2303.11366) | 20 Mar 2023 |
|  | Self-Refine | [Self-Refine: Iterative Refinement with Self-Feedback](https://arxiv.org/abs/2303.17651) | 30 Mar 2023 |
|  | Plan-and-Solve | [Plan-and-Solve Prompting: Improving Zero-Shot Chain-of-Thought Reasoning by Large Language Models](https://arxiv.org/abs/2305.04091) | 6 May 2023 |
|  | PAL | [PAL: Program-aided Language Models](https://arxiv.org/abs/2211.10435) | 18 Nov 2022 |
|  | Reasoning via Planning | [Reasoning with Language Model is Planning with World Model](https://arxiv.org/abs/2305.14992) | 24 May 2023 |
|  | Self-Ask | [Measuring and Narrowing the Compositionality Gap in Language Models](https://arxiv.org/abs/2210.03350) | 7 Oct 2022 |
|  | Least-to-Most Prompting | [Least-to-Most Prompting Enables Complex Reasoning in Large Language Models](https://arxiv.org/abs/2205.10625) | 21 May 2022 |
|  | Self-Polish | [Self-Polish: Enhance Reasoning in Large Language Models via Problem Refinement](https://arxiv.org/abs/2305.14497) | 23 May 2023 |
|  | COMPLEXITY-CoT | [Complexity-Based Prompting for Multi-Step Reasoning](https://arxiv.org/abs/2210.00720) | 3 Oct 2022 |
|  | Maieutic Prompting | [Maieutic Prompting: Logically Consistent Reasoning with Recursive Explanations](https://arxiv.org/abs/2205.11822) | 24 May 2022 |
|  | Algorithm of Thoughts | [Algorithm of Thoughts: Enhancing Exploration of Ideas in Large Language Models](https://arxiv.org/abs/2308.10379) | 20 Aug 2023 |
|  | SuperICL | [Small Models are Valuable Plug-ins for Large Language Models](https://arxiv.org/abs/2305.08848) | 15 May 2023 |
|  | VisualCOMET | [VisualCOMET: Reasoning about the Dynamic Context of a Still Image](https://arxiv.org/abs/2004.10796) | 22 Apr 2020 |
| Memory | MemoryBank | [MemoryBank: Enhancing Large Language Models with Long-Term Memory](https://arxiv.org/abs/2305.10250) | 17 May 2023 |
|  | ChatEval | [ChatEval: Towards Better LLM-based Evaluators through Multi-Agent Debate](https://arxiv.org/abs/2308.07201) | 14 Aug 2023 |
|  | Generative Agents | [Generative Agents: Interactive Simulacra of Human Behavior](https://arxiv.org/abs/2304.03442) | 7 Apr 2023 |
| Planning | SelfCheck | [SelfCheck: Using LLMs to Zero-Shot Check Their Own Step-by-Step Reasoning](https://arxiv.org/abs/2308.00436) | 1 Aug 2023 |
| Automation | APE | [Large Language Models Are Human-Level Prompt Engineers](https://arxiv.org/abs/2211.01910) | 3 Nov 2022 |
| Self-supervised | Self-supervised ICL | [SINC: Self-Supervised In-Context Learning for Vision-Language Tasks](https://arxiv.org/abs/2307.07742) | 15 Jul 2023 |
| Benchmark | BIG-Bench | [Beyond the Imitation Game: Quantifying and extrapolating the capabilities of language models](https://arxiv.org/abs/2206.04615) | 9 Jun 2022 |
|  | ARB | [ARB: Advanced Reasoning Benchmark for Large Language Models](https://arxiv.org/abs/2307.13692) | 25 Jul 2023 |
|  | PlanBench | [PlanBench: An Extensible Benchmark for Evaluating Large Language Models on Planning and Reasoning about Change](https://arxiv.org/abs/2206.10498) | 21 Jun 2022 |
|  | Chain-of-Thought Hub | [Chain-of-Thought Hub: A Continuous Effort to Measure Large Language Models' Reasoning Performance](https://arxiv.org/abs/2305.17306) | 26 May 2023 |
| Survey Paper | A Survey of Chain of Thought Reasoning | [A Survey of Chain of Thought Reasoning: Advances, Frontiers and Future](https://arxiv.org/abs/2309.15402) | 27 Sep 2023 |
|  | Reasoning in Large Language Models | [Towards Reasoning in Large Language Models: A Survey](https://arxiv.org/abs/2212.10403) | 20 Dec 2022 |

## LLM for Agent
| Subcategory | Models | Linked Title | Publication Date |
|-------------|--------|--------------|-----------------|
| Planning | Voyager | [Voyager: An Open-Ended Embodied Agent with Large Language Models](https://arxiv.org/abs/2305.16291) | 25 May 2023 |
|  | DEPS | [Describe, Explain, Plan and Select: Interactive Planning with Large Language Models Enables Open-World Multi-Task Agents](https://arxiv.org/abs/2302.01560) | 3 Feb 2023 |
|  | JARVIS-1 | [JARVIS-1: Open-World Multi-task Agents with Memory-Augmented Multimodal Language Models](https://arxiv.org/abs/2311.05997) | 10 Nov 2023 |
|  | LLM+P | [LLM+P: Empowering Large Language Models with Optimal Planning Proficiency](https://arxiv.org/abs/2304.11477) | 22 Apr 2023 |
|  | Autonomous Agents | [A Survey on Large Language Model based Autonomous Agents](https://arxiv.org/abs/2308.11432) | 22 Aug 2023 |
|  | AgentInstruct | [Agent Instructs Large Language Models to be General Zero-Shot Reasoners](https://arxiv.org/abs/2310.03710) | 5 Oct 2023 |
| Reinforcement Learning | Eureka | [Eureka: Human-Level Reward Design via Coding Large Language Models](https://arxiv.org/abs/2310.12931) | 19 Oct 2023 |
|  | Language to Rewards | [Language to Rewards for Robotic Skill Synthesis](https://arxiv.org/abs/2306.08647) | 14 Jun 2023 |
|  | Language Instructed Reinforcement Learning | [Language Instructed Reinforcement Learning for Human-AI Coordination](https://arxiv.org/abs/2304.07297) | 13 Apr 2023 |
|  | Lafite-RL | [Accelerating Reinforcement Learning of Robotic Manipulations via Feedback from Large Language Models](https://arxiv.org/abs/2311.02379) | 4 Nov 2023 |
|  | ELLM | [Guiding Pretraining in Reinforcement Learning with Large Language Models](https://arxiv.org/abs/2302.06692) | 13 Feb 2023 |
|  | RLAdapter | [RLAdapter: Bridging Large Language Models to Reinforcement Learning in Open Worlds](nan) | nan |
|  | AdaRefiner | [AdaRefiner: Refining Decisions of Language Models with Adaptive Feedback](https://arxiv.org/abs/2309.17176) | 29 Sep 2023 |
|  | Reward Design with Language Models | [Reward Design with Language Models](https://arxiv.org/abs/2303.00001) | 27 Feb 2023 |
|  | EAGER | [EAGER: Asking and Answering Questions for Automatic Reward Shaping in Language-guided RL](https://arxiv.org/abs/2206.09674) | 20 Jun 2022 |
|  | Text2Reward | [Text2Reward: Automated Dense Reward Function Generation for Reinforcement Learning](https://arxiv.org/abs/2309.11489) | 20 Sep 2023 |
| Open-Source Evaluation | AgentSims | [AgentSims: An Open-Source Sandbox for Large Language Model Evaluation](https://arxiv.org/abs/2308.04026) | 8 Aug 2023 |
| Survey Paper | Large Language Model Based Agents | [The Rise and Potential of Large Language Model Based Agents: A Survey](https://arxiv.org/abs/2309.07864) | 14 Sep 2023 |

## LLM for Robots
| Subcategory | Models | Linked Title | Publication Date |
|-------------|--------|--------------|-----------------|
| Multimodal prompts | VIMA | [VIMA: General Robot Manipulation with Multimodal Prompts](https://arxiv.org/abs/2210.03094) | 6 Oct 2022 |
|  | Instruct2Act | [Instruct2Act: Mapping Multi-modality Instructions to Robotic Actions with Large Language Model](https://arxiv.org/abs/2305.11176) | 18 May 2023 |
|  | MOMA-Force | [MOMA-Force: Visual-Force Imitation for Real-World Mobile Manipulation](https://arxiv.org/abs/2308.03624) | 7 Aug 2023 |
| Multimodal LLM | PaLM-E | [PaLM-E: An Embodied Multimodal Language Model](https://arxiv.org/abs/2303.03378) | 6 Mar 2023 |
|  | GATO | [A Generalist Agent](https://arxiv.org/abs/2205.06175) | 12 May 2022 |
|  | Flamingo | [Flamingo: a Visual Language Model for Few-Shot Learning](https://arxiv.org/abs/2204.14198) | 29 Apr 2022 |
|  | Physically Grounded Vision-Language Model | [Physically Grounded Vision-Language Models for Robotic Manipulation](https://arxiv.org/abs/2309.02561) | 5 Sep 2023 |
|  | MOO | [Open-World Object Manipulation using Pre-trained Vision-Language Models](https://arxiv.org/abs/2303.00905) | 2 Mar 2023 |
| Code generation | Code as policies | [Code as Policies: Language Model Programs for Embodied Control](https://arxiv.org/abs/2209.07753) | 16 Sep 2022 |
|  | Progprompt | [ProgPrompt: Generating Situated Robot Task Plans using Large Language Models](https://arxiv.org/abs/2209.11302) | 22 Sep 2022 |
|  | Socratic | [Socratic Models: Composing Zero-Shot Multimodal Reasoning with Language](https://arxiv.org/abs/2204.00598) | 1 Apr 2022 |
|  | SMART-LLM | [SMART-LLM: Smart Multi-Agent Robot Task Planning using Large Language Models](https://arxiv.org/abs/2309.10062) | 18 Sep 2023 |
|  | Statler | [Statler: State-Maintaining Language Models for Embodied Reasoning](https://arxiv.org/abs/2306.17840) | 30 Jun 2023 |
| Decomposing task | SayCan | [Do As I Can, Not As I Say: Grounding Language in Robotic Affordances](https://arxiv.org/abs/2204.01691) | 4 Apr 2022 |
|  | Language Models as Zero-Shot Planners | [Language Models as Zero-Shot Planners: Extracting Actionable Knowledge for Embodied Agents](https://arxiv.org/abs/2201.07207) | 18 Jan 2022 |
|  | SayPlan | [SayPlan: Grounding Large Language Models using 3D Scene Graphs for Scalable Robot Task Planning](https://arxiv.org/abs/2307.06135) | 12 Jul 2023 |
|  | DOREMI | [DoReMi: Grounding Language Model by Detecting and Recovering from Plan-Execution Misalignment](https://arxiv.org/abs/2307.00329) | 1 Jul 2023 |
| Low-level output | SayTap | [SayTap: Language to Quadrupedal Locomotion](https://arxiv.org/abs/2306.07580) | 13 Jun 2023 |
|  | Prompt a Robot to Walk | [Prompt a Robot to Walk with Large Language Models](https://arxiv.org/abs/2309.09969) | 18 Sep 2023 |
| Multimodal Data injection | 3D-LLM | [3D-LLM: Injecting the 3D World into Large Language Models](https://arxiv.org/abs/2307.12981) | 24 Jul 2023 |
|  | LiDAR-LLM | [LiDAR-LLM: Exploring the Potential of Large Language Models for 3D LiDAR Understanding](https://arxiv.org/abs/2312.14074) | 21 Dec 2023 |
| Data generation | Gensim | [GenSim: Generating Robotic Simulation Tasks via Large Language Models](https://arxiv.org/abs/2310.01361) | 2 Oct 2023 |
|  | RoboGen | [RoboGen: Towards Unleashing Infinite Data for Automated Robot Learning via Generative Simulation](https://arxiv.org/abs/2311.01455) | 2 Nov 2023 |
| Planning | Embodied Task Planning | [Embodied Task Planning with Large Language Models](https://arxiv.org/abs/2307.01848) | 4 Jul 2023 |
| Self-improvement | REFLECT | [REFLECT: Summarizing Robot Experiences for Failure Explanation and Correction](https://arxiv.org/abs/2306.15724) | 27 Jun 2023 |
|  | Reflexion | [Reflexion: Language Agents with Verbal Reinforcement Learning](https://arxiv.org/abs/2303.11366) | 20 Mar 2023 |
| Chain of Thought | EmbodiedGPT | [EmbodiedGPT: Vision-Language Pre-Training via Embodied Chain of Thought](https://arxiv.org/abs/2305.15021) | 24 May 2023 |
| Brain | Robotic Brain | [LLM as A Robotic Brain: Unifying Egocentric Memory and Control](https://arxiv.org/abs/2304.09349) | 19 Apr 2023 |
| Survey papers | Toward General-Purpose | [Toward General-Purpose Robots via Foundation Models: A Survey and Meta-Analysis](https://arxiv.org/abs/2312.08782) | 14 Dec 2023 |
|  | Language-conditioned | [Language-conditioned Learning for Robotic Manipulation: A Survey](https://arxiv.org/abs/2312.10807) | 17 Dec 2023 |
|  | Foundation Models | [Foundation Models in Robotics: Applications, Challenges, and the Future](https://arxiv.org/abs/2312.07843) | 13 Dec 2023 |
|  | Robot Learning | [Robot Learning in the Era of Foundation Models: A Survey](https://arxiv.org/abs/2311.14379) | 24 Nov 2023 |
|  | The Development of LLMs | [The Development of LLMs for Embodied Navigation](https://arxiv.org/abs/2311.00530) | 1 Nov 2023 |

## Perception
| Subcategory | Models | Linked Title | Publication Date |
|-------------|--------|--------------|-----------------|
| Object Detection | OWL-ViT | [Simple Open-Vocabulary Object Detection with Vision Transformers](https://arxiv.org/abs/2205.06230) | 12 May 2022 |
|  | GLIP | [Grounded Language-Image Pre-training](https://arxiv.org/abs/2112.03857) | 7 Dec 2021 |
|  | Grounding DINO | [Grounding DINO: Marrying DINO with Grounded Pre-Training for Open-Set Object Detection](https://arxiv.org/abs/2303.05499) | 9 Mar 2023 |
|  | PointCLIP | [PointCLIP: Point Cloud Understanding by CLIP](https://arxiv.org/abs/2112.02413) | 4 Dec 2021 |
|  | Segment Anything | [Segment Anything](https://arxiv.org/abs/2304.02643) | 5 Apr 2023 |
